{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .. datasets sympy numpy matplotlib seaborn -q  # Install dev version of smolagents + some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark date\n",
    "# - set a concrete date:\n",
    "DATE = \"2024-12-26\"\n",
    "# - or use default: today\n",
    "# DATE = None\n",
    "\n",
    "# Evaluation dataset\n",
    "# - the dataset is gated, so you must first visit its page to request access: https://huggingface.co/datasets/smolagents/benchmark-v1\n",
    "EVAL_DATASET = \"GeekAgents/GAIA-web\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and utilities/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def normalize_number_str(number_str: str) -> float:\n",
    "    # we replace these common units and commas to allow\n",
    "    # conversion to float\n",
    "    for char in [\"$\", \"%\", \",\"]:\n",
    "        number_str = number_str.replace(char, \"\")\n",
    "    try:\n",
    "        return float(number_str)\n",
    "    except ValueError:\n",
    "        return float(\"inf\")\n",
    "\n",
    "\n",
    "def split_string(\n",
    "    s: str,\n",
    "    char_list: list[str] = [\",\", \";\"],\n",
    ") -> list[str]:\n",
    "    pattern = f\"[{''.join(char_list)}]\"\n",
    "    return re.split(pattern, s)\n",
    "\n",
    "\n",
    "def is_float(element: any) -> bool:\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def normalize_str(input_str, remove_punct=True) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a string by:\n",
    "    - Removing all white spaces\n",
    "    - Optionally removing punctuation (if remove_punct is True)\n",
    "    - Converting to lowercase\n",
    "    Parameters:\n",
    "    - input_str: str, the string to normalize\n",
    "    - remove_punct: bool, whether to remove punctuation (default: True)\n",
    "    Returns:\n",
    "    - str, the normalized string\n",
    "    \"\"\"\n",
    "    # Remove all white spaces. Required e.g for seagull vs. sea gull\n",
    "    no_spaces = re.sub(r\"\\s\", \"\", input_str)\n",
    "\n",
    "    # Remove punctuation, if specified.\n",
    "    if remove_punct:\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return no_spaces.lower().translate(translator)\n",
    "    else:\n",
    "        return no_spaces.lower()\n",
    "\n",
    "\n",
    "def extract_numbers(text: str) -> List[str]:\n",
    "    \"\"\"This pattern matches:\n",
    "    - Optional negative sign\n",
    "    - Numbers with optional comma thousand separators\n",
    "    - Optional decimal points with decimal numbers\n",
    "    \"\"\"\n",
    "    pattern = r\"-?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?\"\n",
    "\n",
    "    return [el.replace(\",\", \"\") for el in re.findall(pattern, text)]\n",
    "\n",
    "\n",
    "def get_question_score_gaia(\n",
    "    model_answer: str,\n",
    "    ground_truth: str,\n",
    ") -> bool:\n",
    "    \"\"\"Scoring function used to score functions from the GAIA benchmark\"\"\"\n",
    "    if is_float(ground_truth):\n",
    "        normalized_answer = normalize_number_str(str(model_answer))\n",
    "        return normalized_answer == float(ground_truth)\n",
    "\n",
    "    elif any(char in ground_truth for char in [\",\", \";\"]):  # if gt is a list\n",
    "        # question with the fish: normalization removes punct\n",
    "        gt_elems = split_string(ground_truth)\n",
    "        ma_elems = split_string(model_answer)\n",
    "\n",
    "        if len(gt_elems) != len(ma_elems):  # check length is the same\n",
    "            warnings.warn(\"Answer lists have different lengths, returning False.\", UserWarning)\n",
    "            return False\n",
    "\n",
    "        comparisons = []\n",
    "        for ma_elem, gt_elem in zip(ma_elems, gt_elems):  # compare each element as float or str\n",
    "            if is_float(gt_elem):\n",
    "                normalized_ma_elem = normalize_number_str(ma_elem)\n",
    "                comparisons.append(normalized_ma_elem == float(gt_elem))\n",
    "            else:\n",
    "                # we do not remove punct since comparisons can include punct\n",
    "                comparisons.append(\n",
    "                    normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False)\n",
    "                )\n",
    "        return all(comparisons)\n",
    "\n",
    "    else:  # if gt is a str\n",
    "        return normalize_str(model_answer) == normalize_str(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>question</th>\n",
       "      <th>original_question</th>\n",
       "      <th>answer</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>intermediate_steps</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>token_counts</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>In Unlambda, what exact charcter or text needs...</td>\n",
       "      <td>In Unlambda, what exact charcter or text needs...</td>\n",
       "      <td>.</td>\n",
       "      <td>backtick</td>\n",
       "      <td>[TaskStep(task='In Unlambda, what exact charct...</td>\n",
       "      <td>2025-03-31 18:06:16.836019039</td>\n",
       "      <td>2025-03-31 18:07:02.002645969</td>\n",
       "      <td>{'input': 11025, 'output': 313}</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>If we assume all articles published by Nature ...</td>\n",
       "      <td>If we assume all articles published by Nature ...</td>\n",
       "      <td>234</td>\n",
       "      <td>41</td>\n",
       "      <td>[TaskStep(task='If we assume all articles publ...</td>\n",
       "      <td>2025-03-31 18:06:17.334749937</td>\n",
       "      <td>2025-03-31 18:07:06.793097973</td>\n",
       "      <td>{'input': 18094, 'output': 318}</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>A paper about AI regulation that was originall...</td>\n",
       "      <td>A paper about AI regulation that was originall...</td>\n",
       "      <td>Egalitarianism</td>\n",
       "      <td>egalitarian</td>\n",
       "      <td>[TaskStep(task='A paper about AI regulation th...</td>\n",
       "      <td>2025-03-31 18:06:16.415770054</td>\n",
       "      <td>2025-03-31 18:07:50.400644064</td>\n",
       "      <td>{'input': 28536, 'output': 458}</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>If Eliud Kipchoge could maintain his record-ma...</td>\n",
       "      <td>If Eliud Kipchoge could maintain his record-ma...</td>\n",
       "      <td>17000</td>\n",
       "      <td>17</td>\n",
       "      <td>[TaskStep(task='If Eliud Kipchoge could mainta...</td>\n",
       "      <td>2025-03-31 18:07:08.671572924</td>\n",
       "      <td>2025-03-31 18:07:55.145298004</td>\n",
       "      <td>{'input': 17705, 'output': 413}</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>The object in the British Museum's collection ...</td>\n",
       "      <td>The object in the British Museum's collection ...</td>\n",
       "      <td>150</td>\n",
       "      <td>142</td>\n",
       "      <td>[TaskStep(task=\"The object in the British Muse...</td>\n",
       "      <td>2025-03-31 18:07:55.289724827</td>\n",
       "      <td>2025-03-31 18:08:28.414055824</td>\n",
       "      <td>{'input': 11912, 'output': 295}</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_id                                           question  \\\n",
       "0   gpt-4o  In Unlambda, what exact charcter or text needs...   \n",
       "1   gpt-4o  If we assume all articles published by Nature ...   \n",
       "2   gpt-4o  A paper about AI regulation that was originall...   \n",
       "3   gpt-4o  If Eliud Kipchoge could maintain his record-ma...   \n",
       "4   gpt-4o  The object in the British Museum's collection ...   \n",
       "\n",
       "                                   original_question          answer  \\\n",
       "0  In Unlambda, what exact charcter or text needs...               .   \n",
       "1  If we assume all articles published by Nature ...             234   \n",
       "2  A paper about AI regulation that was originall...  Egalitarianism   \n",
       "3  If Eliud Kipchoge could maintain his record-ma...           17000   \n",
       "4  The object in the British Museum's collection ...             150   \n",
       "\n",
       "   true_answer                                 intermediate_steps  \\\n",
       "0     backtick  [TaskStep(task='In Unlambda, what exact charct...   \n",
       "1           41  [TaskStep(task='If we assume all articles publ...   \n",
       "2  egalitarian  [TaskStep(task='A paper about AI regulation th...   \n",
       "3           17  [TaskStep(task='If Eliud Kipchoge could mainta...   \n",
       "4          142  [TaskStep(task=\"The object in the British Muse...   \n",
       "\n",
       "                     start_time                      end_time  \\\n",
       "0 2025-03-31 18:06:16.836019039 2025-03-31 18:07:02.002645969   \n",
       "1 2025-03-31 18:06:17.334749937 2025-03-31 18:07:06.793097973   \n",
       "2 2025-03-31 18:06:16.415770054 2025-03-31 18:07:50.400644064   \n",
       "3 2025-03-31 18:07:08.671572924 2025-03-31 18:07:55.145298004   \n",
       "4 2025-03-31 18:07:55.289724827 2025-03-31 18:08:28.414055824   \n",
       "\n",
       "                      token_counts  level  \n",
       "0  {'input': 11025, 'output': 313}    2.0  \n",
       "1  {'input': 18094, 'output': 318}    2.0  \n",
       "2  {'input': 28536, 'output': 458}    2.0  \n",
       "3  {'input': 17705, 'output': 413}    1.0  \n",
       "4  {'input': 11912, 'output': 295}    2.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "result_dataframes = []\n",
    "for file in glob.glob(\"../../output/*.jsonl\"):\n",
    "    result_dataframes.append(pd.read_json(file, lines=True))\n",
    "result_df = pd.concat(result_dataframes, ignore_index=True)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/ipykernel_16974/4042024437.py:85: UserWarning: Answer lists have different lengths, returning False.\n",
      "  warnings.warn(\"Answer lists have different lengths, returning False.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "result_df[\"is_correct\"] = result_df.apply(lambda x: get_question_score_gaia(x[\"answer\"], x[\"true_answer\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-VL-72B-Instruct</th>\n",
       "      <td>0.162791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>0.288889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              is_correct\n",
       "model_id                                \n",
       "Qwen/Qwen2.5-VL-72B-Instruct    0.162791\n",
       "gpt-4o                          0.288889"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.groupby(\"model_id\")[[\"is_correct\"]].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
